{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c4c55a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Packages\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import Rbf, interp1d\n",
    "\n",
    "# Spatial Handling\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.features import geometry_window\n",
    "from rasterio.transform import from_origin\n",
    "from rasterstats import zonal_stats\n",
    "from affine import Affine\n",
    "import shapely.geometry\n",
    "from shapely.geometry import LineString, box, Point\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from scipy.spatial.distance import euclidean\n",
    "from shapely.ops import nearest_points\n",
    "# import cv2\n",
    "\n",
    "import math\n",
    "import glob\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(category=FutureWarning, action='ignore')\n",
    "warnings.filterwarnings(category=DeprecationWarning, action='ignore')\n",
    "warnings.filterwarnings(category=UserWarning, action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1b16b9",
   "metadata": {},
   "source": [
    "### Env variables preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e3b191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all csv files\n",
    "csv_folder = r\"/home/bkmanu/NIH/spike-1.6.0rc2-linux64/env_covariates/WS_daily_data\"\n",
    "\n",
    "pattern = os.path.join(csv_folder, '*.csv')\n",
    "csv_files = sorted(glob.glob(pattern))\n",
    "\n",
    "df_list = [pd.read_csv(f) for f in csv_files]\n",
    "\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "combined_df['Date'] = pd.to_datetime(combined_df['Date'], errors='coerce')\n",
    "\n",
    "# combined_df['Day']   = combined_df['Date'].dt.day\n",
    "combined_df['Month'] = combined_df['Date'].dt.month\n",
    "combined_df['Year']  = combined_df['Date'].dt.year\n",
    "combined_df['Day'] = combined_df['Date'].dt.dayofyear\n",
    "combined_df['Day_cont'] = (combined_df['Year'] - combined_df['Year'].min()) * 365 + combined_df['Day']\n",
    "# print(combined_df['Year'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615ce2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df['Date'] = pd.to_datetime(combined_df['Date'], errors='coerce')\n",
    "\n",
    "filled_frames = []\n",
    "\n",
    "for station, group in combined_df.groupby('Station_name'):\n",
    "    df = group.sort_values('Date').copy()\n",
    "    x = df['Date'].map(pd.Timestamp.toordinal).values\n",
    "    \n",
    "    for col in ['Temp_max','Temp_avg','Temp_min','RH_max','RH_min']:\n",
    "        y = df[col].values\n",
    "        mask = ~np.isnan(y)\n",
    "        if mask.sum() < 2:\n",
    "            continue\n",
    "        \n",
    "        # linear interpolater\n",
    "        # f_linear = interp1d(\n",
    "        #     x[mask], y[mask],\n",
    "        #     kind='linear',\n",
    "        #     bounds_error=False,\n",
    "        #     fill_value=\"extrapolate\"\n",
    "        # )\n",
    "        # df[col] = f_linear(x)\n",
    "        df[col] = np.interp(x, x[mask], y[mask])\n",
    "        \n",
    "    filled_frames.append(df)\n",
    "\n",
    "interpolated = pd.concat(filled_frames, ignore_index=True)\n",
    "\n",
    "def F_to_C(F):\n",
    "    return (F - 32.0) * 5.0/9.0\n",
    "\n",
    "interpolated['Temp_min_C'] = interpolated['Temp_min'].apply(F_to_C)\n",
    "interpolated['Temp_avg_C'] = interpolated['Temp_avg'].apply(F_to_C)\n",
    "interpolated['Temp_max_C'] = interpolated['Temp_max'].apply(F_to_C)\n",
    "\n",
    "# interpolated.to_csv('new_Maricopa_WS_daily_data1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8e702a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87acf38c",
   "metadata": {},
   "source": [
    "## Create Patches over maricopa county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e65ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Polygon:\n",
    "    @staticmethod\n",
    "    def polygoninitialization(exclude_non_mainland=True, state_init=None, county_name=None):\n",
    "        # Load US county shapefile\n",
    "        shapefile_path = r\"/home/bkmanu/NIH/spike-1.6.0rc2-linux64/Simulated_data/shapefiles/cb_2023_us_county_500k.shp\"\n",
    "        gdf = gpd.read_file(shapefile_path)\n",
    "        gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "        # Filter for only mainland US states if specified\n",
    "        if exclude_non_mainland:\n",
    "            gdf = gdf[(pd.to_numeric(gdf['STATEFP']) < 60) & (pd.to_numeric(gdf['STATEFP']) != 2) & (pd.to_numeric(gdf['STATEFP']) != 15)]\n",
    "\n",
    "        # Filter by specific state by initial\n",
    "        if state_init:\n",
    "            gdf = gdf[gdf['STUSPS'] == str(state_init)]\n",
    "\n",
    "        # Filter by county\n",
    "        if county_name:\n",
    "            gdf = gdf[gdf['NAME'].str.lower() == county_name.lower()]\n",
    "\n",
    "        gdf['STATEFP'] = gdf['STATEFP'].astype(int)\n",
    "        gdf = gdf.drop(columns=['LSAD', 'ALAND', 'AWATER'])\n",
    "        return gdf.reset_index(drop=True)\n",
    "\n",
    "\n",
    "class Grid:\n",
    "    @staticmethod\n",
    "    def create_grid(df, grid_size, mode=\"default\"):\n",
    "        grid_creator = GridCreator(df, grid_size=grid_size, mode=mode)\n",
    "        return grid_creator.create_grid()\n",
    "\n",
    "\n",
    "class GridCreator:\n",
    "    def __init__(self, gdf, grid_size, overlap=True, crs=\"EPSG:4326\", mode=\"default\"):\n",
    "        self.gdf = gdf\n",
    "        self.grid_size = grid_size\n",
    "        self.overlap = overlap\n",
    "        self.crs = crs\n",
    "        self.mode = mode  # New mode parameter to differentiate between methods\n",
    "\n",
    "    def create_grid(self):\n",
    "        lon_min, lat_min, lon_max, lat_max = self.gdf.total_bounds\n",
    "\n",
    "        if self.mode == \"square\":\n",
    "            # Ensuring grid_size x grid_size grid when grid_size = 2 → 2x2 grid\n",
    "            cell_width = (lon_max - lon_min) / self.grid_size\n",
    "            cell_height = (lat_max - lat_min) / self.grid_size\n",
    "\n",
    "            grid_cells = [\n",
    "                box(lon0, lat0, lon0 + cell_width, lat0 + cell_height)\n",
    "                for lat0 in np.linspace(lat_min, lat_max - cell_height, self.grid_size)[::-1]\n",
    "                for lon0 in np.linspace(lon_min, lon_max - cell_width, self.grid_size)\n",
    "            ]\n",
    "\n",
    "        elif self.mode == \"horizontal\":\n",
    "            # split into `grid_size` horizontal strips\n",
    "            cell_height = (lat_max - lat_min) / self.grid_size\n",
    "            grid_cells = [\n",
    "                box(lon_min, lat0, lon_max, lat0 + cell_height)\n",
    "                for lat0 in np.linspace(lat_min, lat_max - cell_height, self.grid_size)[::-1]\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            # Default behavior: creates exactly \"grid_size\" number of cells\n",
    "            step_size = (lon_max - lon_min) / self.grid_size\n",
    "            grid_cells = [\n",
    "                box(lon0, lat_min, lon0 + step_size, lat_max)\n",
    "                for lon0 in np.linspace(lon_min, lon_max - step_size, self.grid_size)\n",
    "            ]\n",
    "\n",
    "        cells = gpd.GeoDataFrame(grid_cells, columns=['geometry'], crs=self.crs)\n",
    "        cells['ID'] = range(1, len(cells) + 1)\n",
    "\n",
    "        # filter grid cells to only those overlapping the area of interest\n",
    "        if self.overlap:\n",
    "            cells = cells.sjoin(self.gdf, how='inner').drop_duplicates('geometry')\n",
    "            cells['ID'] = range(1, len(cells) + 1)\n",
    "\n",
    "        return cells\n",
    "\n",
    "\n",
    "class Adjacency_Polygon:\n",
    "    @classmethod\n",
    "    def calculate_adjacency_matrix(cls, grid):\n",
    "        n = len(grid)\n",
    "        adj_matrix = np.zeros((n, n), dtype=int)\n",
    "\n",
    "        # Moore neighborhood (grid neighbors that share sides/corners)\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                if grid.iloc[i].geometry.touches(grid.iloc[j].geometry):\n",
    "                    adj_matrix[i, j] = 1\n",
    "                    adj_matrix[j, i] = 1\n",
    "\n",
    "        return pd.DataFrame(adj_matrix, index=range(1, n + 1), columns=range(1, n + 1))\n",
    "\n",
    "\n",
    "grid_size = 2\n",
    "\n",
    "# Load all mainland US\n",
    "# us_polygon = Polygon.polygoninitialization(exclude_non_mainland=True)\n",
    "\n",
    "# Create a grid for the US\n",
    "# grid_us_default = Grid.create_grid(us_polygon, grid_size, mode=\"default\")\n",
    "# grid_us_2x2 = Grid.create_grid(us_polygon, grid_size, mode=\"square\")\n",
    "\n",
    "# Load a specific state (e.g., California with FIPS = \"06\")\n",
    "county_polygon = Polygon.polygoninitialization(exclude_non_mainland=True, state_init=\"AZ\", county_name='Maricopa')\n",
    "\n",
    "# Create a grid for Arizona\n",
    "grid1 = Grid.create_grid(county_polygon, grid_size, mode=\"horizontal\")\n",
    "# grid_ca_2x2 = Grid.create_grid(county_polygon, grid_size, mode=\"square\")\n",
    "\n",
    "# Adjacency matrix for a selected grid\n",
    "adj_matrix = Adjacency_Polygon.calculate_adjacency_matrix(grid1)\n",
    "# adj_matrix = Adjacency_Polygon.calculate_adjacency_matrix(grid_ca_2x2)\n",
    "\n",
    "# Display selected grid\n",
    "# grid_ca_2x2\n",
    "\n",
    "grid1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a55437",
   "metadata": {},
   "source": [
    "### Create maricopa county weather stations shape files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804aa4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "maricopa_cty_polygon = Polygon.polygoninitialization(exclude_non_mainland=True, state_init=\"AZ\", county_name=\"Maricopa\")\n",
    "\n",
    "df = pd.read_excel(r\"/home/bkmanu/NIH/spike-1.6.0rc2-linux64/Simulated_data/data/ALERT_sensors_all_by_name.xlsx\")\n",
    "stations_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.Longitude_DD, df.Latitude_DD), crs=\"EPSG:4326\")\n",
    "stations_gdf = stations_gdf.drop(columns=['Dev._Type', 'Installed', 'Device_ID', 'Station_Location'])\n",
    "\n",
    "keep_df = pd.read_excel(r\"/home/bkmanu/NIH/spike-1.6.0rc2-linux64/Simulated_data/data/Weather_Stations_names.xlsx\")\n",
    "keep_df[\"BaseName\"] = (keep_df[\"Station\"].str.split(\",\", n=1).str[0].str.strip())\n",
    "\n",
    "filt_stations_gdf = stations_gdf[stations_gdf[\"Station_Name\"].isin(keep_df[\"BaseName\"])].copy()\n",
    "filt_stations_gdf = filt_stations_gdf.drop_duplicates()\n",
    "filt_stations_gdf = filt_stations_gdf.reset_index(drop=True)\n",
    "\n",
    "maricopa_stations = gpd.sjoin(filt_stations_gdf, maricopa_cty_polygon, how=\"left\")\n",
    "maricopa_stations = maricopa_stations.drop(columns=['index_right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bf67b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = maricopa_cty_polygon.plot(fc=\"none\", ec='black', figsize=(6,6))\n",
    "\n",
    "# Overlay the grid cells\n",
    "grid1.plot(fc=\"none\", ec='green', ax=ax)\n",
    "maricopa_stations.plot(fc='none', ec='blue', ax=ax)\n",
    "\n",
    "for idx, row in grid1.iterrows():\n",
    "    plt.annotate(text=str(row['ID']), xy=(row['geometry'].centroid.x, row['geometry'].centroid.y), color='red', fontsize=15)\n",
    "plt.title('Overlay of Grid cells on Maricopa Cunty')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980595f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "maricopa_stations1 = gpd.sjoin(filt_stations_gdf, maricopa_cty_polygon, how=\"inner\", predicate=\"within\")\n",
    "maricopa_stations1 = maricopa_stations1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb0dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save shape file\n",
    "# out_gpkg = r\"C:\\Users\\manub\\OneDrive - Arizona State University\\Research\\Research_new\\Param_est\\Data_and_shape_files\\Maricopa_cty_WS_shape_files\\Maricopa_WS.gpkg\"\n",
    "\n",
    "# maricopa_cty_polygon.to_file(out_gpkg, layer=\"boundary\", driver=\"GPKG\")\n",
    "# maricopa_stations1.to_file(out_gpkg, layer=\"stations\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09cfdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = maricopa_cty_polygon.plot(fc=\"none\", ec='black', figsize=(6,6))\n",
    "\n",
    "# Overlay the grid cells\n",
    "grid1.plot(fc=\"none\", ec='green', ax=ax)\n",
    "maricopa_stations1.plot(fc='none', ec='blue', ax=ax)\n",
    "\n",
    "for idx, row in grid1.iterrows():\n",
    "    plt.annotate(text=str(row['ID']), xy=(row['geometry'].centroid.x, row['geometry'].centroid.y), color='red', fontsize=15)\n",
    "plt.title('Overlay of Grid cells on Maricopa Cunty')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f533018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a5915eb",
   "metadata": {},
   "source": [
    "### Add environmental layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc4e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_data = pd.read_csv(r\"/home/bkmanu/NIH/spike-1.6.0rc2-linux64/Simulated_data/data/new_Maricopa_WS_daily_data1.csv\")\n",
    "env_data['Station_Name'] = (env_data[\"Station_name\"].str.split(\",\", n=1).str[0].str.strip())\n",
    "env_data = env_data.drop(columns=['Station_name', 'Day', 'Date'])\n",
    "\n",
    "# apply a 7-day centered moving average to your RH_min and RH_max\n",
    "env_data['RH_min_smooth'] = (env_data['RH_min'].rolling(window=7, center=True, min_periods=1).mean())\n",
    "env_data['RH_max_smooth'] = (env_data['RH_max'].rolling(window=7, center=True, min_periods=1).mean())\n",
    "\n",
    "# smooth Tmin and Tmax over a 7-day window\n",
    "env_data['Tmin_smooth'] = (env_data['Temp_min_C'].rolling(window=7, center=True, min_periods=1).mean())\n",
    "env_data['Tmax_smooth'] = (env_data['Temp_max_C'].rolling(window=7, center=True, min_periods=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834f0046",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_env = maricopa_stations1.merge(env_data, on=\"Station_Name\", how=\"inner\")\n",
    "stations_env = stations_env.drop(columns=['index_right', 'COUNTYNS', 'GEOIDFQ', 'NAMELSAD', 'STATE_NAME'])\n",
    "# stations_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9ce9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_in_grid = gpd.sjoin(stations_env, grid1[['ID','geometry']], how='inner', predicate='within')\n",
    "cell_means = (pts_in_grid.groupby(['ID', 'Day_cont'])[['Temp_max','Temp_avg','Temp_min','RH_max','RH_min', 'RH_min_smooth', 'RH_max_smooth', 'Temp_max_C',\n",
    "                'Temp_avg_C', 'Temp_min_C', 'Tmin_smooth', 'Tmax_smooth']].mean().reset_index())\n",
    "grid_daily = grid1.merge(cell_means, on='ID', how='right')\n",
    "grid_daily = grid_daily.drop(columns=['index_right', 'COUNTYNS', 'GEOIDFQ', 'NAMELSAD', 'STATE_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca750f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03dbf349",
   "metadata": {},
   "source": [
    "### Create model Stochastic Petri Net Model (2 Patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf3cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Compute Distance Between Patche centroid for human migration\n",
    "\n",
    "def haversine_distance(coord1, coord2):\n",
    "    R = 3959.0 # 6371.0 Earth's radius in km\n",
    "    lat1, lon1 = map(radians, coord1)\n",
    "    lat2, lon2 = map(radians, coord2)\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    return R * c  # Distance in miles\n",
    "\n",
    "\n",
    "def compute_distance_matrix(grid, use_haversine=True):\n",
    "    num_patches = len(grid)\n",
    "    distance_matrix = np.zeros((num_patches, num_patches))\n",
    "\n",
    "    for i in range(num_patches):\n",
    "        for j in range(i + 1, num_patches):\n",
    "            centroid1 = grid.iloc[i].geometry.centroid\n",
    "            centroid2 = grid.iloc[j].geometry.centroid\n",
    "            \n",
    "            if use_haversine:\n",
    "                dist = haversine_distance((centroid1.y, centroid1.x), (centroid2.y, centroid2.x))\n",
    "            else:\n",
    "                dist = euclidean((centroid1.x, centroid1.y), (centroid2.x, centroid2.y))\n",
    "\n",
    "            distance_matrix[i, j] = dist\n",
    "            distance_matrix[j, i] = dist  # Symmetric matrix\n",
    "\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100dde93",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = compute_distance_matrix(grid1, use_haversine=True)\n",
    "distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6306a4e6",
   "metadata": {},
   "source": [
    "## Vector patch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a02736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "class SIRModelSBML:\n",
    "    def __init__(self):\n",
    "        # Create the root element for the SBML document\n",
    "        self.root = ET.Element(\"{http://www.sbml.org/sbml/level3/version1/core}sbml\", level=\"3\", version=\"1\")\n",
    "        self.model = ET.SubElement(self.root, \"model\", id=\"Model_generated_by_BIOCHAM\")\n",
    "        self.list_of_compartments = ET.SubElement(self.model, \"listOfCompartments\")\n",
    "        self.list_of_species = ET.SubElement(self.model, \"listOfSpecies\")\n",
    "        self.list_of_parameters = ET.SubElement(self.model, \"listOfParameters\")\n",
    "        self.list_of_initial_assignments = ET.SubElement(self.model, \"listOfInitialAssignments\")\n",
    "        self.list_of_reactions = ET.SubElement(self.model, \"listOfReactions\")\n",
    "\n",
    "    def add_compartment(self, compartment_id, spatial_dimensions=\"3\", size=\"1\", constant=\"true\"):\n",
    "        # Add compartment to the model\n",
    "        ET.SubElement(self.list_of_compartments, \"compartment\", id=compartment_id,\n",
    "                      spatialDimensions=spatial_dimensions, size=size, constant=constant)\n",
    "\n",
    "    def add_species(self, species_id, name, compartment, initial_concentration=\"0\", has_only_substance_units=\"false\",\n",
    "                    boundary_condition=\"false\", constant=\"false\"):\n",
    "        # Create species\n",
    "        ET.SubElement(self.list_of_species, \"species\", id=species_id, name=name, compartment=compartment,\n",
    "                      initialConcentration=initial_concentration, hasOnlySubstanceUnits=has_only_substance_units,\n",
    "                      boundaryCondition=boundary_condition, constant=constant)\n",
    "\n",
    "    def add_parameter(self, parameter_id, name, value, constant=\"false\"):\n",
    "        # Create parameter\n",
    "        ET.SubElement(self.list_of_parameters, \"parameter\", id=parameter_id, name=name, value=value, constant=constant)\n",
    "\n",
    "    def add_initial_assignment(self, symbol, math_content):\n",
    "        # Create initial assignment\n",
    "        initial_assignment = ET.SubElement(self.list_of_initial_assignments, \"initialAssignment\", symbol=symbol)\n",
    "        math = ET.SubElement(initial_assignment, \"math\", xmlns=\"http://www.w3.org/1998/Math/MathML\")\n",
    "        math.append(ET.fromstring(math_content))  # Parse MathML from string and append\n",
    "\n",
    "    def add_reaction(self, reaction_id, reversible=\"false\", reactants=None, products=None, kinetic_law_math=None):\n",
    "        # Create reaction\n",
    "        reaction = ET.SubElement(self.list_of_reactions, \"reaction\", id=reaction_id, reversible=reversible, fast=\"false\")\n",
    "\n",
    "        # Add reactants\n",
    "        if reactants:\n",
    "            list_of_reactants = ET.SubElement(reaction, \"listOfReactants\")\n",
    "            for species_id, stoichiometry in reactants.items():\n",
    "                ET.SubElement(list_of_reactants, \"speciesReference\", species=species_id, stoichiometry=str(stoichiometry), constant=\"true\")\n",
    "\n",
    "        # Add products\n",
    "        if products:\n",
    "            list_of_products = ET.SubElement(reaction, \"listOfProducts\")\n",
    "            for species_id, stoichiometry in products.items():\n",
    "                ET.SubElement(list_of_products, \"speciesReference\", species=species_id, stoichiometry=str(stoichiometry), constant=\"true\")\n",
    "\n",
    "        # Create kinetic law with MathML\n",
    "        kinetic_law = ET.SubElement(reaction, \"kineticLaw\")\n",
    "        math = ET.SubElement(kinetic_law, \"math\", xmlns=\"http://www.w3.org/1998/Math/MathML\")\n",
    "        math.append(ET.fromstring(kinetic_law_math))  # Parse MathML from string and append\n",
    "\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    # adj_matrix = AdjacencyMatrixGenerator.generate_adjacency_matrix()\n",
    "    number_of_patches = len(adj_matrix.values[0])\n",
    "    \n",
    "    adjacency_matrix = adj_matrix.values\n",
    "    \n",
    "    # if model_type == \"SIRS\":\n",
    "    sir_model = SIRModelSBML()\n",
    "    \n",
    "    def compute_theta_values(patch_num, neighbor_patch, distance_matrix):\n",
    "        distance = distance_matrix[patch_num - 1, neighbor_patch - 1]\n",
    "\n",
    "        if distance == 0:\n",
    "            return 0, 0, 0  # No self-effect\n",
    "\n",
    "        theta_S = 1 / (10 + distance**2)\n",
    "        theta_I = 1 / (15 + distance**2)\n",
    "        theta_R = 1 / (20 + distance**2)\n",
    "        alpha_S = 1 / (25 + distance**2)\n",
    "        alpha_I = 1 / (50 + distance**2)\n",
    "        return theta_S, theta_I, theta_R, alpha_S, alpha_I\n",
    "    \n",
    "    vector_id = [\"H\", \"M\"]\n",
    "\n",
    "    param_name = \"sigma\"\n",
    "    param_id = f\"{param_name}\"\n",
    "    param_value = \"0.1\"\n",
    "    sir_model.add_parameter(param_id, name=param_name, value=param_value, constant=\"false\")\n",
    "\n",
    "    for param_name in [\"mu\"]:\n",
    "        for v_id, value in [(vector_id[0], \"0.01\")]:\n",
    "            param_id = f\"{param_name}{v_id}\"\n",
    "            sir_model.add_parameter(param_id, name=param_name, value=value, constant=\"false\")\n",
    "\n",
    "    for patch_num in range(1, number_of_patches + 1):\n",
    "        #patch_num = adj_matrix.columns[patch_num-1] # Error for np.where(adjacency_matrix[patch_num - 1] == 1\n",
    "        compartment_id = f\"compartment{patch_num}\"\n",
    "        sir_model.add_compartment(compartment_id, spatial_dimensions=\"3\", size=\"1\", constant=\"true\")\n",
    "\n",
    "        for species_type, initial_value in [(\"S\", 4000), (\"I\", 20), (\"R\", 20), (\"D_S\", 0), (\"D_I\", 0), (\"D_R\", 0)]:\n",
    "            species_id = f\"{species_type}{vector_id[0]}{patch_num}\"\n",
    "            sir_model.add_species(species_id, name=species_id, compartment=compartment_id, initial_concentration=str(initial_value))\n",
    "\n",
    "        for species_type, initial_value in [(\"S\", 2000), (\"I\", 10), (\"D_S\", 0), (\"D_I\", 0)]:\n",
    "            species_id = f\"{species_type}{vector_id[1]}{patch_num}\"\n",
    "            sir_model.add_species(species_id, name=species_id, compartment=compartment_id, initial_concentration=str(initial_value))\n",
    "            \n",
    "        for param_name in [\"beta\"]:\n",
    "            for v1, v2, value in [(vector_id[0], vector_id[1], \"0.3\"), (vector_id[1], vector_id[0], \"0.4\")]:\n",
    "                param_id = f\"{param_name}{v1}{v2}_{patch_num}_{patch_num}\"\n",
    "                sir_model.add_parameter(param_id, name=param_name, value=value, constant=\"false\")\n",
    "        \n",
    "        for param_name in [\"mu\"]:\n",
    "            for v_id, value in [(vector_id[1], \"0.02\")]:\n",
    "                param_id = f\"{param_name}{v_id}_{patch_num}\"\n",
    "                sir_model.add_parameter(param_id, name=param_name, value=value, constant=\"false\")\n",
    "\n",
    "        for state_var in [\"S\", \"I\", \"R\"]:\n",
    "            initial_assignment_symbol = f\"{state_var}{patch_num}\"\n",
    "            sir_model.add_initial_assignment(initial_assignment_symbol, f\"<ci>{state_var}0</ci>\")\n",
    "            \n",
    "        # reactions and kinetic laws WITHIN the patch\n",
    "        kinetic_law_reaction_1 = f\"<ci>MassAction(sigma)</ci>\"\n",
    "        kinetic_law_reaction_2 = f\"<ci>MassAction(beta{vector_id[0]}{vector_id[1]}_{patch_num}_{patch_num})</ci>\"\n",
    "        kinetic_law_reaction_3 = f\"<ci>MassAction(mu{vector_id[0]})</ci>\"\n",
    "\n",
    "        kinetic_law_reaction_4 = f\"<ci>MassAction(beta{vector_id[1]}{vector_id[0]}_{patch_num}_{patch_num})</ci>\"\n",
    "        kinetic_law_reaction_5 = f\"<ci>MassAction(mu{vector_id[1]}_{patch_num})</ci>\"\n",
    "\n",
    "        sir_model.add_reaction(f\"I{vector_id[0]}_{patch_num}_R{vector_id[0]}\", reactants={f\"I{vector_id[0]}{patch_num}\": 1}, products={f\"R{vector_id[0]}{patch_num}\": 1}, kinetic_law_math=kinetic_law_reaction_1)\n",
    "        sir_model.add_reaction(f\"S{vector_id[0]}_{patch_num}_I{vector_id[1]}\", reactants={f\"S{vector_id[0]}{patch_num}\": 1, f\"I{vector_id[1]}{patch_num}\": 1} , products={f\"I{vector_id[0]}{patch_num}\": 1, f\"I{vector_id[1]}{patch_num}\": 1}, kinetic_law_math=kinetic_law_reaction_2)\n",
    "        sir_model.add_reaction(f\"R{vector_id[0]}_{patch_num}\", reactants={f\"R{vector_id[0]}{patch_num}\": 1}, products={f\"D_R{vector_id[0]}{patch_num}\": 1}, kinetic_law_math=kinetic_law_reaction_3)\n",
    "        sir_model.add_reaction(f\"S{vector_id[0]}_{patch_num}\", reactants={f\"S{vector_id[0]}{patch_num}\": 1}, products={f\"D_S{vector_id[0]}{patch_num}\": 1}, kinetic_law_math=kinetic_law_reaction_3)\n",
    "        sir_model.add_reaction(f\"I{vector_id[0]}_{patch_num}\", reactants={f\"I{vector_id[0]}{patch_num}\": 1}, products={f\"D_I{vector_id[0]}{patch_num}\": 1}, kinetic_law_math=kinetic_law_reaction_3)\n",
    "\n",
    "        sir_model.add_reaction(f\"S{vector_id[1]}_{patch_num}_I{vector_id[0]}\", reactants={f\"S{vector_id[1]}{patch_num}\": 1, f\"I{vector_id[0]}{patch_num}\": 1}, products={f\"I{vector_id[1]}{patch_num}\": 1, f\"I{vector_id[0]}{patch_num}\": 1}, kinetic_law_math=kinetic_law_reaction_4)\n",
    "        sir_model.add_reaction(f\"S{vector_id[1]}_{patch_num}\", reactants={f\"S{vector_id[1]}{patch_num}\": 1}, products={f\"D_S{vector_id[1]}{patch_num}\": 1}, kinetic_law_math=kinetic_law_reaction_5)\n",
    "        sir_model.add_reaction(f\"I{vector_id[1]}_{patch_num}\", reactants={f\"I{vector_id[1]}{patch_num}\": 1}, products={f\"D_I{vector_id[1]}{patch_num}\": 1}, kinetic_law_math=kinetic_law_reaction_5)\n",
    "\n",
    "        # reactions and kinetic laws BETWEEN patches\n",
    "        for neighbor_patch in np.where(adjacency_matrix[patch_num - 1] == 1)[0] + 1:\n",
    "            theta_S, theta_I, theta_R, alpha_S, alpha_I = compute_theta_values(patch_num, neighbor_patch, distance_matrix)\n",
    "            sir_model.add_parameter(f\"theta_S{vector_id[0]}_{patch_num}{neighbor_patch}\", name=\"theta_S\", value=str(theta_S), constant=\"false\")\n",
    "            sir_model.add_parameter(f\"theta_I{vector_id[0]}_{patch_num}{neighbor_patch}\", name=\"theta_I\", value=str(theta_I), constant=\"false\")\n",
    "            sir_model.add_parameter(f\"theta_R{vector_id[0]}_{patch_num}{neighbor_patch}\", name=\"theta_R\", value=str(theta_R), constant=\"false\")\n",
    "\n",
    "            sir_model.add_parameter(f\"alpha_S{vector_id[1]}_{patch_num}{neighbor_patch}\", name=\"alpha_S\", value=str(alpha_S), constant=\"false\")\n",
    "            sir_model.add_parameter(f\"alpha_I{vector_id[1]}_{patch_num}{neighbor_patch}\", name=\"alpha_I\", value=str(alpha_I), constant=\"false\")\n",
    "\n",
    "            param_name = \"beta\"\n",
    "            for v1, v2, value in [(vector_id[0], vector_id[1], \"0.2\"), (vector_id[1], vector_id[0], \"0.25\")]:\n",
    "                param_id = f\"{param_name}{v1}{v2}_{patch_num}_{neighbor_patch}\"\n",
    "                sir_model.add_parameter(param_id, name=param_name, value=value, constant=\"false\")\n",
    "\n",
    "            kinetic_law_reaction_between_patches_SH = f\"<ci>MassAction(theta_S{vector_id[0]}_{patch_num}{neighbor_patch})</ci>\"\n",
    "            kinetic_law_reaction_between_patches_IH = f\"<ci>MassAction(theta_I{vector_id[0]}_{patch_num}{neighbor_patch})</ci>\"\n",
    "            kinetic_law_reaction_between_patches_RH = f\"<ci>MassAction(theta_R{vector_id[0]}_{patch_num}{neighbor_patch})</ci>\"\n",
    "\n",
    "            kinetic_law_reaction_between_patches_SM = f\"<ci>MassAction(alpha_S{vector_id[1]}_{patch_num}{neighbor_patch})</ci>\"\n",
    "            kinetic_law_reaction_between_patches_IM = f\"<ci>MassAction(alpha_I{vector_id[1]}_{patch_num}{neighbor_patch})</ci>\"\n",
    "\n",
    "            kinetic_law_reaction_between_patches_SHMII = f\"<ci>MassAction(beta{vector_id[0]}{vector_id[1]}_{patch_num}_{neighbor_patch})</ci>\"\n",
    "            kinetic_law_reaction_between_patches_SMHII = f\"<ci>MassAction(beta{vector_id[1]}{vector_id[0]}_{patch_num}_{neighbor_patch})</ci>\"\n",
    "\n",
    "            sir_model.add_reaction(f\"S{vector_id[0]}_{patch_num}_{neighbor_patch}_S{vector_id[0]}\", reactants={f\"S{vector_id[0]}{patch_num}\": 1}, products={f\"S{vector_id[0]}{neighbor_patch}\": 1}, kinetic_law_math=kinetic_law_reaction_between_patches_SH)\n",
    "            sir_model.add_reaction(f\"I{vector_id[0]}_{patch_num}_{neighbor_patch}_I{vector_id[0]}\", reactants={f\"I{vector_id[0]}{patch_num}\": 1}, products={f\"I{vector_id[0]}{neighbor_patch}\": 1}, kinetic_law_math=kinetic_law_reaction_between_patches_IH)\n",
    "            sir_model.add_reaction(f\"R{vector_id[0]}_{patch_num}_{neighbor_patch}_R{vector_id[0]}\", reactants={f\"R{vector_id[0]}{patch_num}\": 1}, products={f\"R{vector_id[0]}{neighbor_patch}\": 1}, kinetic_law_math=kinetic_law_reaction_between_patches_RH)\n",
    "\n",
    "            sir_model.add_reaction(f\"S{vector_id[1]}_{patch_num}_{neighbor_patch}_S{vector_id[1]}\", reactants={f\"S{vector_id[1]}{patch_num}\": 1}, products={f\"S{vector_id[1]}{neighbor_patch}\": 1}, kinetic_law_math=kinetic_law_reaction_between_patches_SM)\n",
    "            sir_model.add_reaction(f\"I{vector_id[1]}_{patch_num}_{neighbor_patch}_I{vector_id[1]}\", reactants={f\"I{vector_id[1]}{patch_num}\": 1}, products={f\"I{vector_id[1]}{neighbor_patch}\": 1}, kinetic_law_math=kinetic_law_reaction_between_patches_IM)\n",
    "\n",
    "            sir_model.add_reaction(f\"S{vector_id[0]}_{patch_num}_{neighbor_patch}_I{vector_id[1]}\", reactants={f\"S{vector_id[0]}{patch_num}\": 1, f\"I{vector_id[1]}{neighbor_patch}\": 1}, products={f\"I{vector_id[0]}{patch_num}\": 1, f\"I{vector_id[1]}{neighbor_patch}\": 1}, kinetic_law_math=kinetic_law_reaction_between_patches_SHMII)\n",
    "            sir_model.add_reaction(f\"S{vector_id[1]}_{patch_num}_{neighbor_patch}_I{vector_id[0]}\", reactants={f\"S{vector_id[1]}{patch_num}\": 1, f\"I{vector_id[0]}{neighbor_patch}\": 1}, products={f\"I{vector_id[1]}{patch_num}\": 1, f\"I{vector_id[0]}{neighbor_patch}\": 1}, kinetic_law_math=kinetic_law_reaction_between_patches_SMHII)\n",
    "\n",
    "        tree = ET.ElementTree(sir_model.root)\n",
    "        tree.write(\"SIRS_Mosq_2grid.xml\", encoding=\"UTF-8\", xml_declaration=True, method=\"xml\")\n",
    "\n",
    "create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d41b589",
   "metadata": {},
   "source": [
    "### Convert SBML model file to .andl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797dc0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbml_to_andl(input_xml: str, output_andl: str):\n",
    "    \"\"\"\n",
    "    Runs: spike load -f=<input_xml> save -f=<output_andl>\n",
    "    \"\"\"\n",
    "    cmd = [\n",
    "        spike_path,\n",
    "        \"load\", f\"-f={input_xml}\",\n",
    "        \"-net=SPN\",\n",
    "        \"save\", f\"-f={output_andl}\"\n",
    "    ]\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True)\n",
    "        print(f\"Converted {input_xml} -> {output_andl}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Spike conversion failed:\", e)\n",
    "\n",
    "spike_path = r\"/home/bkmanu/NIH/spike-1.6.0rc2-linux64/spike\"\n",
    "sbml_to_andl(\"SIRS_Mosq_2grid.xml\", \"SIRS_Mosq_2grid.andl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aadbb09",
   "metadata": {},
   "source": [
    "## Compute functional Paramters and Run Spike simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbec9e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(r\"/home/bkmanu/NIH/spike-1.6.0rc2-linux64/Simulated_data/SIRS_Mosq_2grid.andl\")\n",
    "OUTPUT_DIR = Path(r\"/home/bkmanu/NIH/spike-1.6.0rc2-linux64/Simulated_data/Simulated_data\")\n",
    "SPIKE_EXE  = Path(r\"/home/bkmanu/NIH/spike-1.6.0rc2-linux64/spike\")\n",
    "\n",
    "# number of underlying constant samples\n",
    "N_samples = 1204\n",
    "\n",
    "# Basis functions\n",
    "def briere(T, a, Tmin, Tmax):\n",
    "    if T <= Tmin or T >= Tmax:\n",
    "        return 0.0\n",
    "    return a * T * (T - Tmin) * math.sqrt(Tmax - T)\n",
    "\n",
    "def logistic(RH, k, RHopt):\n",
    "    return 1.0 / (1.0 + math.exp(-k * (RH - RHopt)))\n",
    "\n",
    "def eyring(T, Psi_ad, AE_ad, R):\n",
    "    Tk = T + 273.15\n",
    "    return Psi_ad * Tk * math.exp(-AE_ad / (R * Tk))\n",
    "\n",
    "def L_eff_multi(RH_min, RH_max, k, RHopt, n):\n",
    "    \"\"\"\n",
    "    Approximate the 24h mean of logistic(RH) by sampling n points\n",
    "    along a half-cosine diurnal curve between RH_min and RH_max.\n",
    "    \"\"\"\n",
    "    ts = np.linspace(0, 1, n)  # fraction of day\n",
    "    RHs = RH_min + (RH_max - RH_min) * 0.5 * (1 - np.cos(2 * np.pi * ts))\n",
    "    return np.mean([logistic(rh, k, RHopt) for rh in RHs])\n",
    "\n",
    "def hourly_avg_briere(Tmean, Tmin_obs, Tmax_obs, a, Tmin, Tmax):\n",
    "    DTR = Tmax_obs - Tmin_obs\n",
    "    hours = np.arange(24)\n",
    "    T_hr = Tmean + (DTR / 2) * np.sin((2 * np.pi / 24) * hours - np.pi/2)\n",
    "    vals = []\n",
    "    for Th in T_hr:\n",
    "        if Th <= Tmin or Th >= Tmax:\n",
    "            vals.append(0.0)\n",
    "        else:\n",
    "            vals.append(a * Th * (Th - Tmin) * math.sqrt(Tmax - Th))\n",
    "    return np.mean(vals)\n",
    "\n",
    "def haversine_distance(coord1, coord2):\n",
    "    R = 3959.0  # Earth radius in miles\n",
    "    lat1, lon1 = radians(coord1[0]), radians(coord1[1])\n",
    "    lat2, lon2 = radians(coord2[0]), radians(coord2[1])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1)*cos(lat2)*sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "def compute_centroid_dist_matrix(grid):\n",
    "    P = len(grid)\n",
    "    D_cent = np.zeros((P, P))\n",
    "    centroids = [(pt.y, pt.x) for pt in grid.geometry.centroid]\n",
    "    for i in range(P):\n",
    "        for j in range(i + 1, P):\n",
    "            d = haversine_distance(centroids[i], centroids[j])\n",
    "            D_cent[i, j] = d\n",
    "            D_cent[j, i] = d\n",
    "    return D_cent\n",
    "\n",
    "def compute_boundary_dist_matrix(grid):\n",
    "    P = len(grid)\n",
    "    D_bound = np.zeros((P, P))\n",
    "    for i in range(P):\n",
    "        for j in range(i + 1, P):\n",
    "            poly_i = grid.geometry.iloc[i]\n",
    "            poly_j = grid.geometry.iloc[j]\n",
    "            p_i, p_j = nearest_points(poly_i, poly_j)\n",
    "            coord_i = (p_i.y, p_i.x)\n",
    "            coord_j = (p_j.y, p_j.x)\n",
    "            d = haversine_distance(coord_i, coord_j)\n",
    "            D_bound[i, j] = d\n",
    "            D_bound[j, i] = d\n",
    "    return D_bound\n",
    "\n",
    "def env_scaler(row, params):\n",
    "    Tmean    = row['Temp_avg_C']\n",
    "    Tmin_obs = row['Tmin_smooth']\n",
    "    Tmax_obs = row['Tmax_smooth']\n",
    "    # RH       = row['RH_avg']\n",
    "\n",
    "    # Transmission Brière\n",
    "    briere_avg = hourly_avg_briere(\n",
    "        Tmean, Tmin_obs, Tmax_obs,\n",
    "        params['a'], params['Tmin'], params['Tmax']\n",
    "    )\n",
    "    B_bar = briere_avg\n",
    "\n",
    "    # Migration Brière (a/10)\n",
    "    a_mig          = params['a'] / 10.0\n",
    "    briere_avg_mig = hourly_avg_briere(\n",
    "        Tmean, Tmin_obs, Tmax_obs,\n",
    "        a_mig, params['Tmin'], params['Tmax']\n",
    "    )\n",
    "    B_bar_mig = briere_avg_mig\n",
    "\n",
    "    # Humidity scaling\n",
    "    # L_rh = logistic(RH, params['k'], params['RHopt'])\n",
    "    RH_min = row['RH_min_smooth']\n",
    "    RH_max = row['RH_max_smooth']\n",
    "    L_rh   = L_eff_multi(RH_min, RH_max, params['k'], params['RHopt'], n=6)\n",
    "\n",
    "    # Eyring mortality\n",
    "    E_unscaled = eyring(Tmean, params['Psi_ad'], params['AE_ad'], params['R'])\n",
    "    E_temp = E_unscaled\n",
    "\n",
    "    return B_bar, B_bar_mig, L_rh, E_temp\n",
    "\n",
    "def compute_within_patch_rates(env_df, params):\n",
    "    \"\"\"\n",
    "    Returns DataFrame with columns:\n",
    "      ['Day','Patch','B_bar','B_bar_mig','L_rh','E_temp','beta_HM','beta_MH','mu_M']\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for _, row in env_df.iterrows():\n",
    "        day   = int(row['Day_cont'])\n",
    "        patch = int(row['ID'])\n",
    "        B_bar, B_bar_mig, L_rh, E_temp = env_scaler(row, params)\n",
    "\n",
    "        # Within-patch β_HM:\n",
    "        beta_HM_ii = (\n",
    "            params['lambda0_HM']\n",
    "            + params['lambda1_HM'] * B_bar\n",
    "            + params['lambda2_HM'] * L_rh\n",
    "            + params['lambda3_HM'] * B_bar * L_rh\n",
    "        )\n",
    "        # Within-patch β_MH:\n",
    "        beta_MH_ii = (\n",
    "            params['gamma0_MH']\n",
    "            + params['gamma1_MH'] * B_bar\n",
    "            + params['gamma2_MH'] * L_rh\n",
    "            + params['gamma3_MH'] * B_bar * L_rh\n",
    "        )\n",
    "        # Mosquito mortality μ_M^(i,i):\n",
    "        mu_M_ii = (\n",
    "            params['p0_mortality']\n",
    "            + params['p1_mortality'] * E_temp\n",
    "        )\n",
    "\n",
    "        rec = {\n",
    "            'Day':     day,\n",
    "            'Patch':   patch,\n",
    "            'B_bar':   B_bar,\n",
    "            'B_bar_mig': B_bar_mig,\n",
    "            'L_rh':    L_rh,\n",
    "            'E_temp':  E_temp,\n",
    "            'beta_HM': beta_HM_ii,\n",
    "            'beta_MH': beta_MH_ii,\n",
    "            'mu_M':    mu_M_ii\n",
    "        }\n",
    "        records.append(rec)\n",
    "\n",
    "    return pd.DataFrame.from_records(\n",
    "        records,\n",
    "        columns=[\n",
    "            'Day','Patch','B_bar','B_bar_mig','L_rh','E_temp',\n",
    "            'beta_HM','beta_MH','mu_M'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def compute_theta_alpha(i, j, D_human, D_mosq, c_vals):\n",
    "    d_h = D_human[i-1, j-1]\n",
    "    theta_S = 1.0 / (c_vals['S'] + d_h**2)\n",
    "    theta_I = 1.0 / (c_vals['I'] + d_h**2)\n",
    "    theta_R = 1.0 / (c_vals['R'] + d_h**2)\n",
    "\n",
    "    d_m = D_mosq[i-1, j-1]\n",
    "    if d_m > 3.0:\n",
    "        alpha_S_raw = 0.0\n",
    "        alpha_I_raw = 0.0\n",
    "    else:\n",
    "        alpha_S_raw = 1.0 / (c_vals['alpha_S'] + d_m**2)\n",
    "        alpha_I_raw = 1.0 / (c_vals['alpha_I'] + d_m**2)\n",
    "\n",
    "    return theta_S, theta_I, theta_R, alpha_S_raw, alpha_I_raw\n",
    "\n",
    "def compute_between_patch_rates(local_rates_df, D_human, D_mosq, c_vals, params):\n",
    "    \"\"\"\n",
    "    Returns DataFrame with columns:\n",
    "      ['Day','From','To','beta_HM','beta_MH','alpha_SM','alpha_IM'].\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    P = D_human.shape[0]\n",
    "\n",
    "    for _, row in local_rates_df.iterrows():\n",
    "        day    = int(row['Day'])\n",
    "        i      = int(row['Patch'])\n",
    "        B_bar  = row['B_bar']\n",
    "        B_bar_mig = row['B_bar_mig']\n",
    "        L_rh   = row['L_rh']\n",
    "\n",
    "        for j in range(1, P+1):\n",
    "            if j == i:\n",
    "                continue\n",
    "\n",
    "            # Between-patch β_HM^(i,j):\n",
    "            beta_HM_ij = (\n",
    "                params['delta0_HM']\n",
    "                + params['delta1_HM'] * B_bar\n",
    "                + params['delta2_HM'] * L_rh\n",
    "                + params['delta3_HM'] * B_bar * L_rh\n",
    "            )\n",
    "            # Between-patch β_MH^(i,j):\n",
    "            beta_MH_ij = (\n",
    "                params['eta0_MH']\n",
    "                + params['eta1_MH'] * B_bar\n",
    "                + params['eta2_MH'] * L_rh\n",
    "                + params['eta3_MH'] * B_bar * L_rh\n",
    "            )\n",
    "\n",
    "            # Compute α‐kernels:\n",
    "            theta_S, theta_I, theta_R, alpha_S_raw, alpha_I_raw = compute_theta_alpha(\n",
    "                i, j, D_human, D_mosq, c_vals\n",
    "            )\n",
    "            alpha_S = alpha_S_raw * B_bar_mig\n",
    "            alpha_I = alpha_I_raw * B_bar_mig\n",
    "\n",
    "            rec = {\n",
    "                'Day':     day,\n",
    "                'From':    i,\n",
    "                'To':      j,\n",
    "                'beta_HM': beta_HM_ij,\n",
    "                'beta_MH': beta_MH_ij,\n",
    "                'alpha_SM': alpha_S,\n",
    "                'alpha_IM': alpha_I\n",
    "            }\n",
    "            records.append(rec)\n",
    "\n",
    "    return pd.DataFrame.from_records(\n",
    "        records,\n",
    "        columns=['Day','From','To','beta_HM','beta_MH','alpha_SM','alpha_IM']\n",
    "    )\n",
    "\n",
    "# write spc and run spike\n",
    "def write_spc_file(spc_path: Path, model_file: Path, output_dir: Path, rates_df, sample_id: int):\n",
    "    \"\"\"Write an SPC that runs all days stepwise with onStep updates.\"\"\"\n",
    "    mf = model_file\n",
    "    od = output_dir\n",
    "\n",
    "    with open(spc_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('/**\\n')\n",
    "        f.write(' * Configuration of Vector Model\\n')\n",
    "        f.write(' */\\n\\n')\n",
    "\n",
    "        # import block\n",
    "        f.write('import: {\\n')\n",
    "        f.write(f'    from: \"{mf}\";\\n')\n",
    "        f.write('}\\n\\n')\n",
    "\n",
    "        # configuration block\n",
    "        f.write('configuration: {\\n\\n')\n",
    "\n",
    "        # model constants\n",
    "        f.write('  model: {\\n')\n",
    "        f.write('    constants: {\\n')\n",
    "        f.write('      parameter: {\\n')\n",
    "        f.write('      }\\n')\n",
    "        f.write('    }\\n')\n",
    "        f.write('    places: {\\n')\n",
    "        f.write('       // SH1: [[1000, 2000]];\\n')\n",
    "        f.write('    }\\n')\n",
    "        f.write('  }\\n\\n')\n",
    "\n",
    "        # simulation \n",
    "        # T = len(rates_df)\n",
    "        f.write('  simulation: {\\n')\n",
    "        f.write('    name: \"SIRIS\";\\n')\n",
    "        f.write('    type: [[\\n')\n",
    "        f.write('      stochastic: {\\n')\n",
    "        f.write('        solver: direct: {\\n')\n",
    "        f.write('           threads: 10;\\n')\n",
    "        f.write('           runs: 2;\\n')\n",
    "        f.write('        }\\n')\n",
    "        f.write('      }\\n')\n",
    "        f.write('    ]];\\n')\n",
    "        f.write('    interval: 0:365:365;\\n\\n')\n",
    "\n",
    "        # Stepwise block\n",
    "        f.write('    onStep: enabled: {\\n')\n",
    "        f.write('       do:{\\n')\n",
    "        for _, row in rates_df.iterrows():\n",
    "            d = int(row['Day'])\n",
    "            f.write(f'      if (simulation.step == {d}) {{ \\n')\n",
    "            for col in rates_df.columns.drop('Day'):\n",
    "                v = row[col]\n",
    "                f.write(f'        constant.{col} = {v:.6e};\\n')\n",
    "            f.write('      }\\n')\n",
    "        f.write('    }\\n')\n",
    "        f.write('  }\\n\\n')\n",
    "        \n",
    "        # export block\n",
    "        filename = f\"sample_{sample_id:02d}_stepwise\"\n",
    "        f.write('    export: {\\n')\n",
    "        f.write('      places: []; // all places\\n')\n",
    "        f.write('      transitions: []; // all transitions\\n')\n",
    "        f.write('      observers: [];\\n')\n",
    "        f.write('      csv: {\\n')\n",
    "        f.write('        sep: \",\"; // Separator\\n')\n",
    "        f.write(f'        file: \"{od}\\\\{filename}\"\\n')\n",
    "        f.write('           << \".csv\";\\n')\n",
    "        f.write('      }\\n')\n",
    "        f.write('    }\\n')\n",
    "        f.write('  }\\n')\n",
    "        f.write('}\\n\\n')\n",
    "\n",
    "        # log block\n",
    "        f.write('log: {\\n')\n",
    "        f.write('  sim.varExa: configuration.simulation.type;\\n')\n",
    "        f.write('}')\n",
    "\n",
    "def run_spike(spike_executable: Path, spc_path: Path) -> None:\n",
    "    '''Write SPC, invoke Spike, then delete the file.'''\n",
    "    cmd = f'\"{spike_executable}\" exe -f=\"{spc_path}\"'\n",
    "    print(f\"[INFO] Running command: {cmd}\")\n",
    "    try:\n",
    "        res = subprocess.run(cmd, check=True, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(\"[INFO] Simulation completed successfully.\")\n",
    "        if res.stdout:\n",
    "            print(res.stdout.decode(errors='ignore'))\n",
    "        if res.stderr:\n",
    "            print(\"[WARN] \", res.stderr.decode(errors='ignore'))\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"[ERROR] Simulation failed (exit code {e.returncode}):\")\n",
    "        print(e.stderr.decode(errors='ignore'))\n",
    "    finally:\n",
    "        spc_path.unlink(missing_ok=True)\n",
    "        print(f\"[INFO] Removed temporary SPC file: {spc_path}\")\n",
    "\n",
    "\n",
    "# check output directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# prepare and load data\n",
    "env_df = grid_daily[grid_daily['Day_cont'] <= 365]\n",
    "P      = len(grid1)                     \n",
    "days   = sorted(env_df['Day_cont'].unique()) \n",
    "T_days = len(days)\n",
    "\n",
    "# Pre‐compute distances & c_vals once\n",
    "D_human = compute_centroid_dist_matrix(grid1)\n",
    "D_mosq  = compute_boundary_dist_matrix(grid1)\n",
    "c_vals  = {'S': 10, 'I': 15, 'R': 20, 'alpha_S': 25, 'alpha_I': 50}\n",
    "\n",
    "# Build a list of rate‐column names\n",
    "rate_col_names = ['Day']\n",
    "for i in range(1, P+1):\n",
    "    rate_col_names.append(f\"betaHM_{i}_{i}\")\n",
    "    rate_col_names.append(f\"betaMH_{i}_{i}\")\n",
    "    rate_col_names.append(f\"muM_{i}\")\n",
    "for i in range(1, P+1):\n",
    "    for j in range(1, P+1):\n",
    "        if i == j:\n",
    "            continue\n",
    "        rate_col_names.append(f\"betaHM_{i}_{j}\")\n",
    "        rate_col_names.append(f\"betaMH_{i}_{j}\")\n",
    "        rate_col_names.append(f\"alpha_SM_{i}{j}\")\n",
    "        rate_col_names.append(f\"alpha_IM_{i}{j}\")\n",
    "\n",
    "# store all constants for each sample s\n",
    "constants_records = []    \n",
    "np.random.seed(42)  # for reproducibility\n",
    "\n",
    "# Ross–Macdonald range\n",
    "beta_min_HM, beta_max_HM = 0.01, 0.80   # mosquito → human\n",
    "beta_min_MH, beta_max_MH = 0.072, 0.64  # human → mosquito\n",
    "Delta_HM = beta_max_HM - beta_min_HM\n",
    "Delta_MH = beta_max_MH - beta_min_MH\n",
    "\n",
    "# Ross–Macdonald mosquito mortality (per day)\n",
    "mu_min, mu_max = 0.05, 0.33\n",
    "Delta_mu = mu_max - mu_min\n",
    "\n",
    "for s in range(N_samples):\n",
    "    print(f\"\\n=== Generating Sample {s} of {N_samples-1} ===\")\n",
    "\n",
    "    # Within‐patch HM\n",
    "    lambda0_HM = beta_min_HM\n",
    "    lambda1_HM = np.random.uniform(0, Delta_HM)\n",
    "    lambda2_HM = np.random.uniform(0, Delta_HM)\n",
    "    lambda3_HM = np.random.uniform(0, Delta_HM)\n",
    "\n",
    "    # Within‐patch MH\n",
    "    gamma0_MH = beta_min_MH\n",
    "    gamma1_MH = np.random.uniform(0, Delta_MH)\n",
    "    gamma2_MH = np.random.uniform(0, Delta_MH)\n",
    "    gamma3_MH = np.random.uniform(0, Delta_MH)\n",
    "\n",
    "    # Between‐patch HM\n",
    "    delta0_HM = beta_min_HM\n",
    "    delta1_HM = np.random.uniform(0, Delta_HM)\n",
    "    delta2_HM = np.random.uniform(0, Delta_HM)\n",
    "    delta3_HM = np.random.uniform(0, Delta_HM)\n",
    "\n",
    "    # Between‐patch MH\n",
    "    eta0_MH   = beta_min_MH\n",
    "    eta1_MH   = np.random.uniform(0, Delta_MH)\n",
    "    eta2_MH   = np.random.uniform(0, Delta_MH)\n",
    "    eta3_MH   = np.random.uniform(0, Delta_MH)\n",
    "\n",
    "    # Mosquito mortality\n",
    "    p0_mortality = mu_min\n",
    "    p1_mortality = np.random.uniform(0, Delta_mu)\n",
    "\n",
    "    # Build a flat dict of constants for this sample s\n",
    "    const_dict = {\n",
    "    # within‐patch HM\n",
    "    \"lambda0_HM\": lambda0_HM,\n",
    "    \"lambda1_HM\": lambda1_HM,\n",
    "    \"lambda2_HM\": lambda2_HM,\n",
    "    \"lambda3_HM\": lambda3_HM,\n",
    "\n",
    "    # within‐patch MH\n",
    "    \"gamma0_MH\":  gamma0_MH,\n",
    "    \"gamma1_MH\":  gamma1_MH,\n",
    "    \"gamma2_MH\":  gamma2_MH,\n",
    "    \"gamma3_MH\":  gamma3_MH,\n",
    "\n",
    "    # between‐patch HM\n",
    "    \"delta0_HM\":  delta0_HM,\n",
    "    \"delta1_HM\":  delta1_HM,\n",
    "    \"delta2_HM\":  delta2_HM,\n",
    "    \"delta3_HM\":  delta3_HM,\n",
    "\n",
    "    # between‐patch MH\n",
    "    \"eta0_MH\":    eta0_MH,\n",
    "    \"eta1_MH\":    eta1_MH,\n",
    "    \"eta2_MH\":    eta2_MH,\n",
    "    \"eta3_MH\":    eta3_MH,\n",
    "\n",
    "    # mosquito mortality\n",
    "    \"p0_mortality\": p0_mortality,\n",
    "    \"p1_mortality\": p1_mortality\n",
    "    }\n",
    "\n",
    "    # Save this sample’s constants for constants.csv\n",
    "    record = {'sample_id': s}\n",
    "    record.update(const_dict)\n",
    "    constants_records.append(record)\n",
    "\n",
    "    # Build params_s for computing rates\n",
    "    params_s = {\n",
    "        'a':      2.71e-4,  'Tmin':   14.67,  'Tmax':   41.0,\n",
    "        'k':      0.1,      'RHopt':  70.0,\n",
    "        'Psi_ad': 13327.0,  'AE_ad':  53135.0,  'R': 8.314,\n",
    "\n",
    "        'lambda0_HM':   lambda0_HM,\n",
    "        'lambda1_HM':   lambda1_HM,\n",
    "        'lambda2_HM':   lambda2_HM,\n",
    "        'lambda3_HM':   lambda3_HM,\n",
    "\n",
    "        'gamma0_MH':    gamma0_MH,\n",
    "        'gamma1_MH':    gamma1_MH,\n",
    "        'gamma2_MH':    gamma2_MH,\n",
    "        'gamma3_MH':    gamma3_MH,\n",
    "\n",
    "        'delta0_HM':    delta0_HM,\n",
    "        'delta1_HM':    delta1_HM,\n",
    "        'delta2_HM':    delta2_HM,\n",
    "        'delta3_HM':    delta3_HM,\n",
    "\n",
    "        'eta0_MH':      eta0_MH,\n",
    "        'eta1_MH':      eta1_MH,\n",
    "        'eta2_MH':      eta2_MH,\n",
    "        'eta3_MH':      eta3_MH,\n",
    "\n",
    "        'p0_mortality': p0_mortality,\n",
    "        'p1_mortality': p1_mortality\n",
    "    }\n",
    "\n",
    "    # Compute within‐patch rates for all days 1..T_days\n",
    "    local_rates_df_s = compute_within_patch_rates(env_df, params_s)\n",
    "\n",
    "    # Compute between‐patch rates\n",
    "    between_rates_df_s = compute_between_patch_rates(local_rates_df_s, D_human, D_mosq, c_vals, params_s)\n",
    "\n",
    "    # Build a DataFrame rates_df\n",
    "    rows = []\n",
    "    for d in days:\n",
    "        daily_dict = {'Day': d}\n",
    "        # within‐patch\n",
    "        df_loc_d = local_rates_df_s[local_rates_df_s['Day'] == d]\n",
    "        for i in range(1, P+1):\n",
    "            row_i = df_loc_d[df_loc_d['Patch'] == i].iloc[0]\n",
    "            daily_dict[f\"betaHM_{i}_{i}\"] = row_i['beta_HM']\n",
    "            daily_dict[f\"betaMH_{i}_{i}\"] = row_i['beta_MH']\n",
    "            daily_dict[f\"muM_{i}\"]        = row_i['mu_M']\n",
    "\n",
    "        # between‐patch\n",
    "        df_betw_d = between_rates_df_s[between_rates_df_s['Day'] == d]\n",
    "        for i in range(1, P+1):\n",
    "            for j in range(1, P+1):\n",
    "                if i == j: \n",
    "                    continue\n",
    "                row_ij = df_betw_d[(df_betw_d['From'] == i) & (df_betw_d['To'] == j)].iloc[0]\n",
    "                daily_dict[f\"betaHM_{i}_{j}\"]  = row_ij['beta_HM']\n",
    "                daily_dict[f\"betaMH_{i}_{j}\"]  = row_ij['beta_MH']\n",
    "                daily_dict[f\"alpha_SM_{i}{j}\"] = row_ij['alpha_SM']\n",
    "                daily_dict[f\"alpha_IM_{i}{j}\"] = row_ij['alpha_IM']\n",
    "\n",
    "        rows.append(daily_dict)\n",
    "\n",
    "    rates_df = pd.DataFrame(rows, columns=rate_col_names)\n",
    "\n",
    "    # Create a subfolder for this sample’s Spike outputs\n",
    "    sample_folder = OUTPUT_DIR / f\"sample_{s:02d}\"\n",
    "    sample_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".spc\", delete=False) as tmp:\n",
    "        spc_file = Path(tmp.name)\n",
    "\n",
    "    # Write one SPC that runs all days stepwise and run it\n",
    "    write_spc_file(spc_file, MODEL_PATH, sample_folder, rates_df, sample_id=s)\n",
    "    run_spike(SPIKE_EXE, spc_file)\n",
    "\n",
    "    print(f\"✅ Sample {s}: stepwise simulation complete ({len(rates_df)} days)\")\n",
    "\n",
    "# After all N_samples done, write out constants.csv\n",
    "constants_df = pd.DataFrame(constants_records)\n",
    "constants_df.to_csv(OUTPUT_DIR / \"constants.csv\", index=False)\n",
    "print(f\"\\nAll done! Wrote constants.csv and Spike outputs under:\\n  {OUTPUT_DIR}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7657108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
